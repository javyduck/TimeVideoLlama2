{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed4b0f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1, Loss: 188017.1875\n",
      "Epoch 11, Loss: 192212.6875\n",
      "Epoch 21, Loss: 166953.234375\n",
      "Epoch 31, Loss: 121098.765625\n",
      "Epoch 41, Loss: 91882.65625\n",
      "Epoch 51, Loss: 34399.6015625\n",
      "Epoch 61, Loss: 20037.244140625\n",
      "Epoch 71, Loss: 2647.69677734375\n",
      "Epoch 81, Loss: 847.4603881835938\n",
      "Epoch 91, Loss: 245.80917358398438\n",
      "Epoch 101, Loss: 120.67447662353516\n",
      "Epoch 111, Loss: 76.60591125488281\n",
      "Epoch 121, Loss: 75.31448364257812\n",
      "Epoch 131, Loss: 58.4202995300293\n",
      "Epoch 141, Loss: 36.890445709228516\n",
      "Epoch 151, Loss: 27.06822967529297\n",
      "Epoch 161, Loss: 26.44301986694336\n",
      "Epoch 171, Loss: 25.467485427856445\n",
      "Epoch 181, Loss: 17.445430755615234\n",
      "Epoch 191, Loss: 16.05217933654785\n",
      "Epoch 201, Loss: 12.730245590209961\n",
      "Epoch 211, Loss: 13.048492431640625\n",
      "Epoch 221, Loss: 10.823512077331543\n",
      "Epoch 231, Loss: 12.70936393737793\n",
      "Epoch 241, Loss: 7.634136199951172\n",
      "Epoch 251, Loss: 7.192852973937988\n",
      "Epoch 261, Loss: 6.437993049621582\n",
      "Epoch 271, Loss: 6.107530117034912\n",
      "Epoch 281, Loss: 5.287620544433594\n",
      "Epoch 291, Loss: 6.574556350708008\n",
      "Epoch 301, Loss: 17.898235321044922\n",
      "Epoch 311, Loss: 16.241975784301758\n",
      "Epoch 321, Loss: 14.028247833251953\n",
      "Epoch 331, Loss: 28.791067123413086\n",
      "Epoch 341, Loss: 25.673261642456055\n",
      "Epoch 351, Loss: 18.579689025878906\n",
      "Epoch 361, Loss: 8.008919715881348\n",
      "Epoch 371, Loss: 4.3038153648376465\n",
      "Epoch 381, Loss: 2.36578106880188\n",
      "Epoch 391, Loss: 2.6645851135253906\n",
      "Epoch 401, Loss: 5.178106784820557\n",
      "Epoch 411, Loss: 2.661659002304077\n",
      "Epoch 421, Loss: 66.9305191040039\n",
      "Epoch 431, Loss: 10.006304740905762\n",
      "Epoch 441, Loss: 2.3930389881134033\n",
      "Epoch 451, Loss: 7.583087921142578\n",
      "Epoch 461, Loss: 3.4239985942840576\n",
      "Epoch 471, Loss: 3.0339791774749756\n",
      "Epoch 481, Loss: 3.526287078857422\n",
      "Epoch 491, Loss: 3.1095352172851562\n",
      "Epoch 501, Loss: 3.252742052078247\n",
      "Epoch 511, Loss: 1.6470998525619507\n",
      "Epoch 521, Loss: 1.4149012565612793\n",
      "Epoch 531, Loss: 1.6609572172164917\n",
      "Epoch 541, Loss: 7.3475799560546875\n",
      "Epoch 551, Loss: 3.0503082275390625\n",
      "Epoch 561, Loss: 6.756752014160156\n",
      "Epoch 571, Loss: 5.533914089202881\n",
      "Epoch 581, Loss: 2.3668644428253174\n",
      "Epoch 591, Loss: 1.69963538646698\n",
      "Epoch 601, Loss: 20.880538940429688\n",
      "Epoch 611, Loss: 7.26091194152832\n",
      "Epoch 621, Loss: 179.4771270751953\n",
      "Epoch 631, Loss: 71.60247802734375\n",
      "Epoch 641, Loss: 12.633350372314453\n",
      "Epoch 651, Loss: 8.274258613586426\n",
      "Epoch 661, Loss: 3.250972270965576\n",
      "Epoch 671, Loss: 7.85910701751709\n",
      "Epoch 681, Loss: 6.033352851867676\n",
      "Epoch 691, Loss: 5.785308361053467\n",
      "Epoch 701, Loss: 4.8969879150390625\n",
      "Epoch 711, Loss: 19.795747756958008\n",
      "Epoch 721, Loss: 4.6582536697387695\n",
      "Epoch 731, Loss: 15.997272491455078\n",
      "Epoch 741, Loss: 3.1189446449279785\n",
      "Epoch 751, Loss: 7.907474994659424\n",
      "Epoch 761, Loss: 8.811924934387207\n",
      "Epoch 771, Loss: 3.7274930477142334\n",
      "Epoch 781, Loss: 4.976959705352783\n",
      "Epoch 791, Loss: 30.3039608001709\n",
      "Epoch 801, Loss: 50.40583038330078\n",
      "Epoch 811, Loss: 13.91596508026123\n",
      "Epoch 821, Loss: 1.1477288007736206\n",
      "Epoch 831, Loss: 7.935168743133545\n",
      "Epoch 841, Loss: 3.1619713306427\n",
      "Epoch 851, Loss: 7.26188850402832\n",
      "Epoch 861, Loss: 9.302343368530273\n",
      "Epoch 871, Loss: 11.861215591430664\n",
      "Epoch 881, Loss: 5.705709934234619\n",
      "Epoch 891, Loss: 5.6601176261901855\n",
      "Epoch 901, Loss: 3.015343427658081\n",
      "Epoch 911, Loss: 5.331576824188232\n",
      "Epoch 921, Loss: 6.402864933013916\n",
      "Epoch 931, Loss: 160.72817993164062\n",
      "Epoch 941, Loss: 30.367597579956055\n",
      "Epoch 951, Loss: 18.963714599609375\n",
      "Epoch 961, Loss: 2.3737564086914062\n",
      "Epoch 971, Loss: 14.484423637390137\n",
      "Epoch 981, Loss: 5.187902927398682\n",
      "Epoch 991, Loss: 92.85320281982422\n",
      "Test Loss: 0.027939973399043083\n"
     ]
    }
   ],
   "source": [
    "import setGPU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def generate_data(size, ranges):\n",
    "    data = []\n",
    "    for _range in ranges:\n",
    "        data.append(np.random.uniform(_range[0], _range[1], size // len(ranges)))\n",
    "    return torch.tensor(np.concatenate(data), dtype=torch.float32).to(device)\n",
    "\n",
    "class CustomAutoencoder(nn.Module):\n",
    "    def __init__(self, M, N, L, P, K):\n",
    "        super(CustomAutoencoder, self).__init__()\n",
    "        self.matrix_MN = nn.Parameter(torch.randn(M, N))\n",
    "        self.coefficient_net = nn.Sequential(\n",
    "            nn.Linear(1, M//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M//4, M//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M//2, M)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(N, N),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N, N),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N, K)\n",
    "        )\n",
    "        \n",
    "        self.matrix_LP = nn.Parameter(torch.randn(L, P))\n",
    "        self.selector = nn.Sequential(\n",
    "            nn.Linear(K, K),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(K, K),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(K, L),\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.dynamic_mlp = nn.Sequential(\n",
    "            nn.Linear(P + K, 100),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "        self.temperature = 5.0\n",
    "\n",
    "\n",
    "    def forward(self, x, temperature=0.5):\n",
    "        # Ensure x is batched properly\n",
    "        x = x.view(-1, 1)  # Correcting shape for batch processing\n",
    "        coeffs = self.coefficient_net(x)\n",
    "        coeffs = F.softmax(coeffs, dim=1)\n",
    "        # Ensure matrix multiplication compatibility\n",
    "        x_encoded = torch.matmul(coeffs, self.matrix_MN)  # coeffs [batch_size, M], matrix_MN [M, N]\n",
    "\n",
    "        x_bottleneck = self.encoder(x_encoded)\n",
    "        \n",
    "        logits = self.selector(x_bottleneck)\n",
    "        if self.training:\n",
    "            # Use Gumbel-Softmax during training for differentiability\n",
    "            probs = F.gumbel_softmax(logits, tau=self.temperature, hard=True, dim=-1)\n",
    "        else:\n",
    "            # Use argmax during evaluation for deterministic results\n",
    "            _, max_indices = torch.max(logits, dim=1)\n",
    "            probs = torch.zeros_like(logits).scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "        selected_row = torch.matmul(probs, self.matrix_LP)  # probs [batch_size, L], matrix_LP [L, P]\n",
    "        mlp_input = torch.cat((selected_row, x_bottleneck), dim=1)\n",
    "        x_reconstructed = self.dynamic_mlp(mlp_input)\n",
    "        \n",
    "        return x_reconstructed, probs\n",
    "    \n",
    "    def update_temperature(self, epoch, total_epochs, minimum_temperature=0.01):\n",
    "        # Linear annealing\n",
    "        self.temperature = max(minimum_temperature, self.temperature - (self.temperature - minimum_temperature) * (epoch / total_epochs))\n",
    "\n",
    "# Hyperparameters\n",
    "M = 100\n",
    "N = 100\n",
    "K = 100\n",
    "L = 1\n",
    "P = 100\n",
    "\n",
    "# Model, Optimizer, Loss\n",
    "model = CustomAutoencoder(M, N, L, P, K).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training data preparation\n",
    "train_size = 10000\n",
    "input_ranges = [(-100, -10), (-0.5, 0.5), (10, 20), (10, 100)]\n",
    "inputs = generate_data(train_size, input_ranges).unsqueeze(1)  # Reshape for batch processing\n",
    "targets = inputs.clone()  # Targets are the same as inputs\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "batch_size = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = generate_data(batch_size, input_ranges).unsqueeze(1)\n",
    "    targets = inputs.clone()\n",
    "    batch_inputs = inputs\n",
    "    batch_targets = targets\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs, gumbel_probs = model(batch_inputs)\n",
    "    model.update_temperature(epoch, num_epochs)\n",
    "    loss = 100 * criterion(outputs, batch_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "test_size = 1000\n",
    "test_inputs = generate_data(test_size, input_ranges).unsqueeze(1)\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_outputs, gumbel_probs= model(test_inputs)\n",
    "    test_loss = criterion(test_outputs, test_inputs)\n",
    "#     print(\"Test outputs:\", test_outputs)\n",
    "    print(\"Test Loss:\", test_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1953e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "x = F.gumbel_softmax(torch.tensor([1,1,0.5],device = 'cuda', dtype=torch.half), tau=1, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2a702083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c043e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index = random.sample(range(len(test_inputs)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9aa811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.9407],\n",
       "        [43.5708],\n",
       "        [65.2328],\n",
       "        [81.1902],\n",
       "        [16.3516]], device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63acbe11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.8965],\n",
       "        [43.5963],\n",
       "        [65.3737],\n",
       "        [81.5181],\n",
       "        [16.2908]], device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b877d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_probs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9bdd09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_probs[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "73a81228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 6],\n",
       "        [11],\n",
       "        [14],\n",
       "        [17]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gather(index = indices, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e783955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dea35398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x.parameters())[0][-10:].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4354074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.weight[-10:].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff54e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tensor\n",
    "tensor = torch.LongTensor([3236502, 3236501])\n",
    "\n",
    "# Compute the integer parts\n",
    "integer_parts = tensor // 100  # Integer division\n",
    "\n",
    "# Compute the fractional parts\n",
    "fractional_parts = (tensor % 100) / 100  # Modulus operation followed by division\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c155f9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0200, 0.0100])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractional_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37aaa6c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 08:56:39.105694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 08:56:39.105769: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 08:56:39.105791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 08:56:39.111782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea913e2c10b40fd8ca694927fd7796f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../checkpoints/VideoLLaMA2-7B were not used when initializing Videollama2MistralForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']\n",
      "- This IS expected if you are initializing Videollama2MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Videollama2MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Videollama2MistralForCausalLM were not initialized from the model checkpoint at ../checkpoints/VideoLLaMA2-7B and are newly initialized: ['time_mlp.linear1.bias', 'time_mlp.linear1.weight', 'time_mlp.linear2.bias', 'time_mlp.linear2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import transformers\n",
    "from videollama2 import conversation as conversation_lib\n",
    "from videollama2.constants import NUM_FRAMES, IGNORE_INDEX, MMODAL_TOKEN_INDEX, DEFAULT_MMODAL_TOKEN, DEFAULT_MMODAL_START_TOKEN, DEFAULT_MMODAL_END_TOKEN\n",
    "from videollama2.videollama2_trainer import VideoLLaMA2Trainer\n",
    "from videollama2.model import *\n",
    "import torch\n",
    "config = transformers.AutoConfig.from_pretrained('../checkpoints/VideoLLaMA2-7B', trust_remote_code=True)\n",
    "config._attn_implementation = None\n",
    "config.mm_use_time_token = True\n",
    "config.float_token_id_start = 0\n",
    "config.float_token_id_end = 1\n",
    "\n",
    "\n",
    "            \n",
    "model = Videollama2MistralForCausalLM.from_pretrained(\n",
    "    '../checkpoints/VideoLLaMA2-7B',\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    do_sample=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4567db0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(40000, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b5b2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_model().config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6ad8e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.3000e+01, 1.0160e+03, 1.8720e+03, 3.7920e+03, 7.8080e+03, 7.8080e+03,\n",
    "        1.3376e+04, 2.8800e+04, 2.8672e+04, 1.3000e+01, 1.0368e+04, 9.3440e+03,\n",
    "        2.8200e+02, 2.0000e+03, 1.8720e+03, 1.7664e+04, 2.2656e+04, 6.3600e+02,\n",
    "        3.4800e+02, 2.8800e+04, 1.9584e+04, 2.8800e+04, 2.8672e+04, 3.2000e+04,\n",
    "        3.2256e+04, 3.2256e+04, 3.2256e+04, 3.2256e+04, 3.2256e+04, 3.2256e+04,\n",
    "        3.2256e+04, 3.2384e+04, 1.3000e+01, 6.1120e+03, 2.8672e+04, 1.3760e+03,\n",
    "        2.8800e+04, 2.8672e+04, 3.2000e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04,\n",
    "        3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2384e+04, 1.3000e+01,\n",
    "        4.0320e+03, 7.7120e+03, 3.5200e+02, 2.8800e+04, 2.8672e+04, 3.2000e+04,\n",
    "        3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04,\n",
    "        3.2128e+04, 3.2384e+04, 1.3000e+01, 1.9712e+04, 2.8800e+04, 2.8672e+04,\n",
    "        3.2000e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04,\n",
    "        3.2256e+04, 3.2256e+04, 3.2384e+04, 1.3000e+01, 3.2000e+03, 3.4800e+02,\n",
    "        2.7200e+02, 2.9920e+03, 3.0200e+02, 2.4576e+04, 1.2560e+03, 2.8800e+04,\n",
    "        7.3200e+02, 2.8800e+04, 1.6320e+04, 2.8800e+04, 4.1600e+02, 1.2560e+03,\n",
    "        1.3056e+04, 1.0000e+03, 2.8672e+04, 2.0000e+00, 7.3200e+02, 1.6320e+04,\n",
    "        2.8800e+04, 4.3200e+03, 1.2320e+03, 2.7200e+02, 2.4576e+04, 1.2560e+03,\n",
    "        2.5440e+03, 4.5600e+02, 2.8800e+04, 7.3200e+02, 2.8800e+04, 1.6320e+04,\n",
    "        2.8800e+04, 1.0960e+03, 3.7800e+02, 2.8800e+04, 2.8672e+04, 2.4960e+03,\n",
    "        2.6400e+02, 1.1040e+03, 1.5280e+03, 2.8672e+04, 2.8672e+04, 2.0000e+00,\n",
    "        7.3200e+02, 1.6320e+04, 2.8800e+04, 1.9072e+04, 8.4800e+02, 2.7200e+02,\n",
    "        2.6080e+03, 7.5200e+03, 3.5400e+02, 1.6800e+03, 4.0960e+03, 2.8672e+04,\n",
    "        7.3200e+02, 2.8800e+04, 1.6320e+04, 2.8800e+04, 1.9584e+04, 2.8800e+04,\n",
    "        2.8672e+04, 3.2000e+04, 3.2256e+04, 3.2384e+04, 1.9712e+04, 2.8800e+04,\n",
    "        2.8672e+04, 3.2000e+04, 3.2256e+04, 3.2384e+04, 2.8672e+04, 2.0000e+00,\n",
    "        0.0000e+00, 0.0000e+00], device='cuda:0', dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d911cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x1 = torch.tensor([32334.2], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a647210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31984.], dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([31990]).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "93e5c7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,  True,\n",
       "         True,  True, False, False, False,  True,  True,  True, False, False,\n",
       "        False, False], device='cuda:0')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32001 <= x) & (x <= 32384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "11c261b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3000e+01, 1.0160e+03, 1.8720e+03, 3.7920e+03, 7.8080e+03, 7.8080e+03,\n",
       "        1.3376e+04, 2.8800e+04, 2.8672e+04, 1.3000e+01, 1.0368e+04, 9.3440e+03,\n",
       "        2.8200e+02, 2.0000e+03, 1.8720e+03, 1.7664e+04, 2.2656e+04, 6.3600e+02,\n",
       "        3.4800e+02, 2.8800e+04, 1.9584e+04, 2.8800e+04, 2.8672e+04, 3.2000e+04,\n",
       "        3.2256e+04, 3.2256e+04, 3.2256e+04, 3.2256e+04, 3.2256e+04, 3.2256e+04,\n",
       "        3.2256e+04, 3.2384e+04, 1.3000e+01, 6.1120e+03, 2.8672e+04, 1.3760e+03,\n",
       "        2.8800e+04, 2.8672e+04, 3.2000e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04,\n",
       "        3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2384e+04, 1.3000e+01,\n",
       "        4.0320e+03, 7.7120e+03, 3.5200e+02, 2.8800e+04, 2.8672e+04, 3.2000e+04,\n",
       "        3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04,\n",
       "        3.2128e+04, 3.2384e+04, 1.3000e+01, 1.9712e+04, 2.8800e+04, 2.8672e+04,\n",
       "        3.2000e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04, 3.2128e+04,\n",
       "        3.2256e+04, 3.2256e+04, 3.2384e+04, 1.3000e+01, 3.2000e+03, 3.4800e+02,\n",
       "        2.7200e+02, 2.9920e+03, 3.0200e+02, 2.4576e+04, 1.2560e+03, 2.8800e+04,\n",
       "        7.3200e+02, 2.8800e+04, 1.6320e+04, 2.8800e+04, 4.1600e+02, 1.2560e+03,\n",
       "        1.3056e+04, 1.0000e+03, 2.8672e+04, 2.0000e+00, 7.3200e+02, 1.6320e+04,\n",
       "        2.8800e+04, 4.3200e+03, 1.2320e+03, 2.7200e+02, 2.4576e+04, 1.2560e+03,\n",
       "        2.5440e+03, 4.5600e+02, 2.8800e+04, 7.3200e+02, 2.8800e+04, 1.6320e+04,\n",
       "        2.8800e+04, 1.0960e+03, 3.7800e+02, 2.8800e+04, 2.8672e+04, 2.4960e+03,\n",
       "        2.6400e+02, 1.1040e+03, 1.5280e+03, 2.8672e+04, 2.8672e+04, 2.0000e+00,\n",
       "        7.3200e+02, 1.6320e+04, 2.8800e+04, 1.9072e+04, 8.4800e+02, 2.7200e+02,\n",
       "        2.6080e+03, 7.5200e+03, 3.5400e+02, 1.6800e+03, 4.0960e+03, 2.8672e+04,\n",
       "        7.3200e+02, 2.8800e+04, 1.6320e+04, 2.8800e+04, 1.9584e+04, 2.8800e+04,\n",
       "        2.8672e+04, 3.2000e+04, 3.2256e+04, 3.2384e+04, 1.9712e+04, 2.8800e+04,\n",
       "        2.8672e+04, 3.2000e+04, 3.2256e+04, 3.2384e+04, 2.8672e+04, 2.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32001<=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4dec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, input_dim = 4096, hidden_dim = 4096, dropout_rate=0.1):\n",
    "        super(FlattenHead, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, 1) \n",
    "        nn.init.xavier_uniform_(self.linear2.weight, gain=nn.init.calculate_gain('sigmoid'))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7ed7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlattenHead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8cbec32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.ones(5, 10, 4096)\n",
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0bfcad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    '../checkpoints/VideoLLaMA2-7B',\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d4c941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '1', '0', '.', '0', ',', '▁-', '8', '.', '5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('10.0, -8.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd467e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tokenizer length: 32000\n",
      "New tokenizer length: 32002\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    '../checkpoints/VideoLLaMA2-7B',\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "# Print the initial length of the tokenizer\n",
    "print(\"Initial tokenizer length:\", len(tokenizer))\n",
    "\n",
    "# Add the special token\n",
    "special_tokens_dict = {'additional_special_tokens': ['<t_start>', '<t_end>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Print the length of the tokenizer after adding the special token\n",
    "print(\"New tokenizer length:\", len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e7514d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '../checkpoints/VideoLLaMA2-7B',\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# # Now wrap it with your custom tokenizer\n",
    "# time_tokenizer = TimeTokenizer(original_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34a36e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "import bisect\n",
    "import re\n",
    "import torch\n",
    "\n",
    "class TimeTokenizer:\n",
    "    def __init__(self, tokenizer, range_min=-50, range_max=50, bins=25):\n",
    "        \n",
    "        # Store the original tokenizer, the legacy has to be set to False for correct decoding\n",
    "        tokenizer.legacy = False\n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "        # Generate and add new tokens based on the given range and bin size\n",
    "        self.range_min = range_min\n",
    "        self.range_max = range_max\n",
    "        self.bins = bins\n",
    "        self.range_tokens = [f\"{x:.2f}\" for x in np.linspace(range_min, range_max, bins+1)]\n",
    "        \n",
    "        vocab = tokenizer.get_vocab()\n",
    "        if '<t_start>' not in vocab:\n",
    "            tokenizer.add_tokens(['<t_start>'])\n",
    "            tokenizer.add_tokens(self.range_tokens)\n",
    "            tokenizer.add_tokens(['<t_end>'])\n",
    "            self.num_new_tokens = len(self.range_tokens) + 2\n",
    "        else:\n",
    "            self.num_new_tokens = 0\n",
    "            \n",
    "        # Cache necessary token ids\n",
    "        self.time_id_start = tokenizer.convert_tokens_to_ids('<t_start>')\n",
    "        self.float_token_id_start = tokenizer.convert_tokens_to_ids(self.range_tokens[0])\n",
    "        self.float_token_id_end = tokenizer.convert_tokens_to_ids(self.range_tokens[-1])\n",
    "        self.time_id_end = tokenizer.convert_tokens_to_ids('<t_end>')\n",
    "        \n",
    "        self.sorted_tokens = torch.tensor(sorted(float(val) for val in self.range_tokens), dtype=torch.float32)\n",
    "        self.float_to_token = {float(val): val for val in self.range_tokens}\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.unk_token = tokenizer.unk_token\n",
    "        self.model_max_length = tokenizer.model_max_length\n",
    "        self.padding_side = tokenizer.padding_side\n",
    "        \n",
    "    def __call__(self, text, **kwargs):\n",
    "        \n",
    "        return_tensors = kwargs.pop('return_tensors', None)  # Default to False if not specified\n",
    "            \n",
    "        # Encoding the text\n",
    "        new_input_ids = self.encode(text, **kwargs)\n",
    "        \n",
    "        # Generating an attention mask that matches the new input IDs\n",
    "        attention_mask =[1] * len(new_input_ids)\n",
    "        if return_tensors == 'pt':\n",
    "            new_input_ids = torch.tensor(new_input_ids, dtype=torch.long)\n",
    "            \n",
    "        return SimpleNamespace(**{'input_ids': new_input_ids, 'attention_mask': None})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tokenizer)\n",
    "    \n",
    "    def encode(self, text, **kwargs):\n",
    "\n",
    "        # Split text into parts to handle both special tags and general text\n",
    "        parts = re.split(r'(<t_start>|<t_end>)', text)\n",
    "        new_input_ids = []\n",
    "\n",
    "        add_special_tokens = kwargs.get('add_special_tokens', True)  # Default to False if not specified\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(parts):\n",
    "            part = parts[i]\n",
    "            if part == '<t_start>':\n",
    "                new_input_ids.append(self.time_id_start)\n",
    "                i += 1 \n",
    "                if i < len(parts):\n",
    "                    new_indices = self.process_inner_text(parts[i])\n",
    "                    new_input_ids.extend(new_indices)\n",
    "                    i += 1\n",
    "            elif part == '<t_end>':\n",
    "                new_input_ids.append(self.time_id_end)\n",
    "                i += 1\n",
    "            else:\n",
    "                ### tmp fix for extrac space problem ###\n",
    "                token_ids = self._tokenizer.encode('\\n' + parts[i], add_special_tokens=False)[2:]\n",
    "                new_input_ids.extend(token_ids)\n",
    "                i += 1\n",
    "\n",
    "        if add_special_tokens:\n",
    "            \n",
    "            # Retrieve special tokens ids based on configuration in tokenizer\n",
    "            special_tokens = []\n",
    "            if hasattr(self._tokenizer, 'add_bos_token') and self._tokenizer.add_bos_token:\n",
    "                bos_token_id = self._tokenizer.bos_token_id if hasattr(self._tokenizer, 'bos_token_id') else None\n",
    "                if bos_token_id is not None:\n",
    "                    special_tokens.append(bos_token_id)\n",
    "\n",
    "            if hasattr(self._tokenizer, 'add_eos_token') and self._tokenizer.add_eos_token:\n",
    "                eos_token_id = self._tokenizer.eos_token_id if hasattr(self._tokenizer, 'eos_token_id') else None\n",
    "                if eos_token_id is not None:\n",
    "                    special_tokens.append(eos_token_id)\n",
    "\n",
    "            # Insert BOS token at the beginning if present\n",
    "            if special_tokens and self._tokenizer.add_bos_token:\n",
    "                new_input_ids.insert(0, special_tokens[0])\n",
    "\n",
    "            # Append EOS token at the end if present\n",
    "            if special_tokens and self._tokenizer.add_eos_token:\n",
    "                new_input_ids.append(special_tokens[-1])\n",
    "\n",
    "        return torch.tensor(new_input_ids, dtype = torch.float32)\n",
    "\n",
    "    def process_inner_text(self, text):\n",
    "        # Extract all numbers from the text as a PyTorch tensor for vectorized operations\n",
    "        floats = torch.tensor([float(num) for num in re.split(r'\\s*,\\s*', text.strip('[]')) if num], dtype = torch.float32)\n",
    "        # Clip the input values to be within the specified range\n",
    "        floats_clipped = torch.clamp(floats, min=self.range_min, max=self.range_max)\n",
    "\n",
    "        # Convert range_tokens to a tensor if not already\n",
    "        range_tokens_floats = [float(token) for token in self.range_tokens]\n",
    "        range_tokens_tensor = torch.tensor(range_tokens_floats, dtype=torch.float32)\n",
    "\n",
    "        # Find the indices for each number using searchsorted\n",
    "        positions = torch.searchsorted(range_tokens_tensor, floats_clipped, right=True) - 1\n",
    "        positions = torch.clamp(positions, 0, len(self.range_tokens) - 2)  # Ensure indices are within the bounds\n",
    "\n",
    "        # Lower and upper token values\n",
    "        lower_tokens = range_tokens_tensor[positions]\n",
    "        upper_tokens = range_tokens_tensor[positions + 1]\n",
    "\n",
    "        # Calculate the interpolated indices\n",
    "        lower_indices = self.float_token_id_start + positions\n",
    "\n",
    "        # Compute the interpolated token indices\n",
    "        fractional_part = (floats_clipped - lower_tokens) / (upper_tokens - lower_tokens)\n",
    "        interpolated_indices = lower_indices + fractional_part\n",
    "\n",
    "        return interpolated_indices\n",
    "\n",
    "\n",
    "    def batch_decode(self, sequences, **kwargs):\n",
    "        return [\n",
    "            self.decode(\n",
    "                seq,\n",
    "                **kwargs,\n",
    "            )\n",
    "            for seq in sequences\n",
    "        ]\n",
    "\n",
    "    def decode(self, token_ids, **kwargs):\n",
    "        if not len(token_ids):\n",
    "            return \"\"\n",
    "        \n",
    "        # Ensure input is in a consistent format (using PyTorch for potential GPU support)\n",
    "        if isinstance(token_ids, list) or isinstance(token_ids, np.ndarray):\n",
    "            token_ids = torch.tensor(token_ids, dtype=torch.float32)\n",
    "        # No need to convert if already a torch tensor\n",
    "        # No need to convert if already a torch tensor\n",
    "        \n",
    "        if token_ids.device.type == 'cuda':\n",
    "            device = token_ids.device\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        \n",
    "        # Create a mask for float tokens using torch operations\n",
    "        is_float = (self.float_token_id_start <= token_ids) & (token_ids <= self.float_token_id_end)\n",
    "\n",
    "        # Find boundaries of chunks by changes in the mask\n",
    "        diff = torch.diff(is_float.to(torch.int), dim=0)\n",
    "        boundaries = torch.nonzero(diff, as_tuple=False).squeeze() + 1\n",
    "        chunk_indices = torch.cat([torch.tensor([0], device=device), boundaries, torch.tensor([token_ids.size(0)], device=device)])\n",
    "\n",
    "        # Process each chunk based on its type\n",
    "        decoded_parts = []\n",
    "        for i in range(chunk_indices.size(0) - 1):\n",
    "            chunk = token_ids[chunk_indices[i]:chunk_indices[i+1]]\n",
    "            if is_float[chunk_indices[i]]:\n",
    "\n",
    "                # Interpolate float values\n",
    "                base_ids = chunk.long() - self.float_token_id_start\n",
    "                lower_bounds = torch.tensor([float(self.range_tokens[idx]) for idx in base_ids], device=device)\n",
    "                upper_bounds = torch.tensor([float(self.range_tokens[min(idx + 1, len(self.range_tokens) - 1)]) for idx in base_ids], device=device)\n",
    "                fractional_parts = chunk.frac()\n",
    "                interpolated_floats = lower_bounds + fractional_parts * (upper_bounds - lower_bounds)\n",
    "                float_strs = [f\"{num.item():.2f}\" for num in interpolated_floats]\n",
    "                decoded_parts.append(\", \".join(float_strs))\n",
    "            else:\n",
    "                if isinstance(chunk, list):\n",
    "                    chunk = list(map(int, chunk))\n",
    "                else:\n",
    "                    chunk = chunk.long()\n",
    "                # Decode non-float tokens using the tokenizer (assumes moving to CPU for compatibility)\n",
    "                decoded_text = self._tokenizer.decode(chunk, **kwargs)\n",
    "                decoded_parts.append(decoded_text)\n",
    "        return \"\".join(decoded_parts)\n",
    "    \n",
    "    def token_to_float(self, token_ids):\n",
    "        device = token_ids.device\n",
    "        dtype = torch.float32\n",
    "        \n",
    "        # Ensure token_ids is a 2D tensor even if it's not batched\n",
    "        if token_ids.ndim == 1:\n",
    "            token_ids = token_ids.unsqueeze(0)  # Reshape to [1, n]\n",
    "\n",
    "        # Calculate base indices for each token\n",
    "        base_ids = (token_ids // 100).long() - self.float_token_id_start\n",
    "\n",
    "        # Collect lower and upper bounds from pre-defined ranges\n",
    "        lower_bounds = torch.tensor([float(self.range_tokens[idx]) for idx in base_ids.flatten()], dtype=torch.bfloat16, device=device).view_as(base_ids)\n",
    "        upper_bounds = torch.tensor([float(self.range_tokens[min(idx + 1, len(self.range_tokens) - 1)]) for idx in base_ids.flatten()], dtype=torch.bfloat16, device=device).view_as(base_ids)\n",
    "\n",
    "        # Calculate fractional parts for interpolation\n",
    "        fractional_parts = (token_ids % 100).float() / 100\n",
    "\n",
    "        # Perform interpolation\n",
    "        interpolated_floats = lower_bounds + fractional_parts * (upper_bounds - lower_bounds)\n",
    "\n",
    "        # Match the original shape of token_ids\n",
    "        if interpolated_floats.ndim == 2 and interpolated_floats.shape[0] == 1:\n",
    "            interpolated_floats = interpolated_floats.squeeze(0)  # Remove batch dimension if it was originally unbatched\n",
    "\n",
    "        return interpolated_floats\n",
    "    \n",
    "    \n",
    "    def float_to_tokenid(self, floats):\n",
    "        # Clip the input values to be within the specified range\n",
    "        floats_clipped = torch.clamp(floats, min=-180, max=180)\n",
    "\n",
    "        # Convert range_tokens to a tensor if not already\n",
    "        range_tokens_floats = [float(token) for token in self.range_tokens]\n",
    "        range_tokens_tensor = torch.tensor(range_tokens_floats, device=floats.device, dtype=torch.float32)\n",
    "\n",
    "        # Find the indices for each number using searchsorted\n",
    "        positions = torch.searchsorted(range_tokens_tensor, floats_clipped, right=True) - 1\n",
    "        positions = torch.clamp(positions, 0, len(self.range_tokens) - 2)  # Ensure indices are within the bounds\n",
    "\n",
    "        # Lower and upper token values\n",
    "        lower_tokens = range_tokens_tensor[positions]\n",
    "        upper_tokens = range_tokens_tensor[positions + 1]\n",
    "\n",
    "        # Calculate the interpolated indices\n",
    "        lower_indices = self.float_token_id_start + positions\n",
    "\n",
    "        # Compute the interpolated token indices\n",
    "        fractional_part = (floats_clipped - lower_tokens) / (upper_tokens - lower_tokens)\n",
    "        interpolated_indices = (100 * (lower_indices + fractional_part)).long()\n",
    "\n",
    "        return interpolated_indices\n",
    "\n",
    "# Example usage\n",
    "original_tokenizer = AutoTokenizer.from_pretrained('../checkpoints/VideoLLaMA2-7B', use_fast=False)\n",
    "time_tokenizer = TimeTokenizer(original_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ce6a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all numbers from the text as a numpy array for vectorized operations\n",
    "text = '14.79'\n",
    "numbers = np.array([float(num) for num in re.split(r'\\s*,\\s*', text.strip('[]')) if num])\n",
    "\n",
    "# Find the indices for each number using vectorized search\n",
    "positions = np.searchsorted(time_tokenizer.sorted_tokens, numbers, side='right') - 1\n",
    "positions = np.clip(positions, 0, len(time_tokenizer.sorted_tokens) - 2)  # Ensure indices are within the bounds\n",
    "\n",
    "# Lower and upper token values\n",
    "lower_tokens = np.array(time_tokenizer.range_tokens)[positions]\n",
    "upper_tokens = np.array(time_tokenizer.range_tokens)[positions + 1]\n",
    "\n",
    "# Calculate the interpolated indices\n",
    "lower_indices = time_tokenizer.float_token_id_start + positions\n",
    "upper_indices = time_tokenizer.float_token_id_start + positions + 1\n",
    "\n",
    "# Compute the interpolated token indices\n",
    "interpolated_indices = lower_indices + ((numbers - lower_tokens.astype(float)) / \n",
    "                                        (upper_tokens.astype(float) - lower_tokens.astype(float))) * (upper_indices - lower_indices)\n",
    "#         return  interpolated_indices.tolist()\n",
    "return [int(round(num * 100)) for num in interpolated_indices.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de907560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32011.7395])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_indices'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c73b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_tokenizer.float_token_id_end - time_tokenizer.float_token_id_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "378374c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3236300])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3236300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "366a8e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32384"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "506*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7946cd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.ones(5).cuda().float()\n",
    "\n",
    "is_float=(5 <= token_ids) & (token_ids <= 1)\n",
    "is_float.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e611f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1536, 28725, 28740, 28750, 28770]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_tokenizer('time,123').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5df22cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa69dd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32363"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b1c5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer.legacy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ecea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.utils  import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04107e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.logits_process.InfNanRemoveLogitsProcessor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InfNanRemoveLogitsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d99900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers/generation\n",
    "/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dd2ff985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4912ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''he current video records driving scenario: <video>\\n Control Signal until current Frame Sequence is: Speed: <t_start>[14.79, 12.25, 8.65, 6.35, 5.28, 4.92, 4.825]<t_end>\\n Curvature: <t_start>[-0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0]<t_end>\\n Acceleration: <t_start>[-0.74, -2.74, -2.45, -1.42, -0.56, -0.18, -0.08]<t_end>\\n Course: <t_start>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]<t_end>\\nWhat is the action of ego car?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef964c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he current video records driving scenario: <video>\n",
      " Control Signal until current Frame Sequence is: Speed: <t_start>[14.79, 12.25, 8.65, 6.35, 5.28, 4.92, 4.825]<t_end>\n",
      " Curvature: <t_start>[-0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0]<t_end>\n",
      " Acceleration: <t_start>[-0.74, -2.74, -2.45, -1.42, -0.56, -0.18, -0.08]<t_end>\n",
      " Course: <t_start>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]<t_end>\n",
      "What is the action of ego car?\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81c8e34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6975])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor([14.79]) -12)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e90b3b05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time_tokenizer.encode(text, add_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cbe5f07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6500e+02, 1.8680e+03, 3.7980e+03, 7.8060e+03, 7.8100e+03, 1.3406e+04,\n",
       "        2.8747e+04, 5.2300e+02, 9.8310e+03, 2.8767e+04, 1.3000e+01, 1.0336e+04,\n",
       "        9.3150e+03, 2.8200e+02, 1.9960e+03, 1.8680e+03, 1.7624e+04, 2.2716e+04,\n",
       "        6.3600e+02, 3.4900e+02, 2.8747e+04, 1.9586e+04, 2.8747e+04, 2.8705e+04,\n",
       "        3.2000e+04, 3.2017e+04, 3.2017e+04, 3.2016e+04, 3.2015e+04, 3.2015e+04,\n",
       "        3.2015e+04, 3.2015e+04, 3.2027e+04, 1.3000e+01, 6.1190e+03, 2.8728e+04,\n",
       "        1.3730e+03, 2.8747e+04, 2.8705e+04, 3.2000e+04, 3.2014e+04, 3.2014e+04,\n",
       "        3.2014e+04, 3.2014e+04, 3.2014e+04, 3.2014e+04, 3.2014e+04, 3.2027e+04,\n",
       "        1.3000e+01, 4.0350e+03, 7.7080e+03, 3.5200e+02, 2.8747e+04, 2.8705e+04,\n",
       "        3.2000e+04, 3.2013e+04, 3.2013e+04, 3.2013e+04, 3.2013e+04, 3.2013e+04,\n",
       "        3.2013e+04, 3.2013e+04, 3.2027e+04, 1.3000e+01, 1.9688e+04, 2.8747e+04,\n",
       "        2.8705e+04, 3.2000e+04, 3.2014e+04, 3.2014e+04, 3.2014e+04, 3.2014e+04,\n",
       "        3.2014e+04, 3.2014e+04, 3.2014e+04, 3.2027e+04, 1.3000e+01, 3.1950e+03,\n",
       "        3.4900e+02, 2.7200e+02, 2.9920e+03, 3.0200e+02, 2.4630e+04, 1.2530e+03,\n",
       "        2.8804e+04])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68e27f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he current video records driving scenario: <video>\n",
      " Control Signal until current Frame Sequence is: Speed:  <t_start>14.79, 12.25, 8.65, 6.35, 5.28, 4.92, 4.83<t_end> \n",
      " Curvature:  <t_start>0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00<t_end> \n",
      " Acceleration:  <t_start>-0.74, -2.74, -2.45, -1.42, -0.56, -0.18, -0.08<t_end> \n",
      " Course:  <t_start>0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00<t_end> \n",
      "What is the action of ego car?\n"
     ]
    }
   ],
   "source": [
    "t2 = time_tokenizer.decode(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67d4421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he current video records driving scenario: <video>\n",
      " Control Signal until current Frame Sequence is: Speed:  <t_start>14.80, 12.20, 8.60, 6.40, 5.20, 5.00, 4.80<t_end> \n",
      " Curvature:  <t_start>0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00<t_end> \n",
      " Acceleration:  <t_start>-0.80, -2.80, -2.40, -1.40, -0.60, -0.20, 0.00<t_end> \n",
      " Course:  <t_start>0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00<t_end> \n",
      "What is the action of ego car?\n"
     ]
    }
   ],
   "source": [
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7ce87b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3235900, 3219950, 3219937, 3219937, 3219925, 3219937, 3219937])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_tokenizer.float_to_tokenid(torch.tensor([178.4700,  18.4500,  18.4000,  18.3500,  18.3100,  18.3500,  18.3900], dtype=torch.bfloat16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragdriver",
   "language": "python",
   "name": "ragdriver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
